{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c811ad39-5d9c-4a98-a85c-876866cee1b1",
   "metadata": {},
   "source": [
    "# Build locality terms from the iDigBio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a9f6b-7934-492f-ba7a-533c16bbafec",
   "metadata": {},
   "source": [
    "This is data that I gleaned from a raw iDigBio dump for gazetteer input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f7b613-0718-4d47-aa68-98dc815034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import html\n",
    "import sqlite3\n",
    "import unicodedata as uni\n",
    "from collections import defaultdict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from flora.pylib.traits import terms as f_terms\n",
    "from tqdm.notebook import tqdm\n",
    "from traiter.pylib import term_util as tu\n",
    "from traiter.pylib.pipes import extensions, tokenizer\n",
    "from traiter.pylib.traits import terms as t_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52eb3ce8-6ddb-45db-abee-79a818066b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will definitely need to change these directories\n",
    "INPUT_DATA_DIR = Path(\"..\") / \"..\" / \"..\" / \"digi-leap\" / \"gazetteer\" / \"data\"\n",
    "INPUT_DB = INPUT_DATA_DIR / \"gazetteer_04_idigbio_2020-03-30.db\"\n",
    "FIELDS = \"\"\"\n",
    "    locality continent country countryCode county higherGeography island islandGroup\n",
    "    locationRemarks municipality stateProvince waterBody\n",
    "    \"\"\".split()\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\" / \"idigbio\"\n",
    "TEMP_DIR = DATA_DIR / \"temp\"\n",
    "DB = DATA_DIR / \"localities.sqlite\"\n",
    "LOCALITIES = DATA_DIR / \"locality_terms.csv\"\n",
    "\n",
    "PROCESSES = 16  # Number of parallel processes\n",
    "\n",
    "# Spacy POS tags to skip\n",
    "POS_ALIAS = \" ADP CCONJ DET NUM PUNCT SCONJ SYM \".split()\n",
    "\n",
    "# These should be handled by ADP above but are not (modified from Wikipedia)\n",
    "SKIP = set(\n",
    "    \"\"\"\n",
    "    a aboard about abt. above abreast absent across after against along aloft alongside\n",
    "    amid amidst mid midst among amongst anti apropos around round as aslant astride\n",
    "    at @ atop ontop bar barring before behind below beneath neath beside besides\n",
    "    between 'tween beyond but by chez circa c. ca. come concerning contra counting\n",
    "    cum despite spite down during effective ere except excepting excluding failing\n",
    "    following for from in including inside into less like minus modulo mod near\n",
    "    nearer nearest next notwithstanding of o' off offshore on onto opposite out\n",
    "    outside over o'er pace past pending per plus post pre pro qua re regarding\n",
    "    respecting sans save saving short since sub than through thru throughout\n",
    "    thruout till times to touching toward, towards under underneath unlike until\n",
    "    unto up upon versus vs. v. via vice vis-à-vis wanting with w/ w. c̄ within\n",
    "    w/i without 'thout w/o\n",
    "    \"\"\".split()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33a35c6-459e-43c7-b583-8028421b28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"loc add error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3391-3e42-42fc-b8f4-240bfb14283e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978c3136-9415-487d-9626-ee860f920b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions.add_extensions()\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"ner\"])\n",
    "tokenizer.setup_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caa1a9-d45e-4c72-b306-c3f1986870e5",
   "metadata": {},
   "source": [
    "## Basic locality normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf2e57-5dc4-4dd1-99db-ddd50c6263d6",
   "metadata": {},
   "source": [
    "See [here](https://en.wikipedia.org/wiki/Unicode_character_property) for a description of the character class abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad21444-32f4-4967-b2ea-bd4216818625",
   "metadata": {},
   "source": [
    "The raw localities are rough, perform some simple steps to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ee5c24-5faf-44e3-904b-d1e80a468aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOO_SHORT = 3\n",
    "\n",
    "PUNCT = \"\"\"[&%$#!*,/;.:?'\"_-]+\"\"\"\n",
    "\n",
    "SUBS = [\n",
    "    # Agressively remove brackets\n",
    "    (re.compile(r\" [()\\[\\]\\{\\}]+ \", flags=re.X), \" \"),\n",
    "    # Like (...)\n",
    "    (re.compile(rf\"\\(+ {PUNCT} \\)+\", flags=re.X), \" \"),\n",
    "    # Like (9)\n",
    "    (re.compile(r\"\\(+ \\s* \\d* \\s* \\)+ \", flags=re.X), \" \"),\n",
    "    # Lat/long\n",
    "    (re.compile(r\"\\(? [\\d.-]+ [\\s,]+ [\\d.-]+ \\)?\", flags=re.X), \" \"),\n",
    "    # CSV delimiters? The question marks are odd, I admit\n",
    "    (re.compile(r\"[.,?]{2,}\"), \" \"),\n",
    "    # Enclosing quotes\n",
    "    (re.compile(r\"\"\"^ [({\\['\"/] \\s* (.+) \\s* [\\]})'\"/] $\"\"\", flags=re.X), r\"\\1\"),\n",
    "    # Leading PUNCT\n",
    "    (re.compile(rf\"^( \\s* {PUNCT} \\s* )+\", flags=re.X), \" \"),\n",
    "    # Trailing PUNCT\n",
    "    (re.compile(rf\"( \\s* {PUNCT} \\s* )+ $\", flags=re.X), \" \"),\n",
    "    # Handle contractions & possesives\n",
    "    (re.compile(r\" \\s ( '[st] ) \", flags=re.X), r\"\\1\"),\n",
    "    # Handle abbreviations\n",
    "    (re.compile(r\" ([\\p{L}\\p{M}]{1,4}) \\s ( \\. ) \", flags=re.X), r\"\\1\\2\"),\n",
    "    # Handle periods\n",
    "    (re.compile(r\" ([\\p{L}\\p{M}]{5,}) ( \\. ) \", flags=re.X), r\"\\1 \\2\"),\n",
    "    # Remove back slashes\n",
    "    (re.compile(r\"\\\\\", flags=re.X), \"\"),\n",
    "]\n",
    "\n",
    "# Character classes\n",
    "CONTROLS = \" Cc Cf Cs Co Cn \".split()  # All control characters\n",
    "SYMBOLS = \" Sc \".split()  # Currency symbols\n",
    "SEPARATORS = \" Zl Zp \".split()  # Line & paragraph separators\n",
    "REMOVE = CONTROLS + SYMBOLS + SEPARATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe956a2-0d25-49e8-802a-c4701f9a406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute(loc, subs):\n",
    "    prev = \"\"\n",
    "    while prev != loc:\n",
    "        prev = loc\n",
    "        for regexp, repl in subs:\n",
    "            loc = regexp.sub(repl, loc)\n",
    "            loc = loc.strip()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bd6976-f1a7-45a0-a9f7-fd28ffb35c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_location(loc):\n",
    "    try:\n",
    "        # Replace HTML entities\n",
    "        loc = html.unescape(loc)\n",
    "\n",
    "        # Lower case the string\n",
    "        loc = loc.lower()\n",
    "\n",
    "        # Remove control characters & some punct\n",
    "        loc = [\" \" if uni.category(c) in REMOVE else c for c in loc]\n",
    "        loc = \"\".join(loc)\n",
    "\n",
    "        # Normalize chars to ASCII\n",
    "        loc = uni.normalize(\"NFKD\", loc)\n",
    "\n",
    "        # Do some replacements\n",
    "        loc = substitute(loc, SUBS)\n",
    "\n",
    "        # Normalize spaces\n",
    "        loc = \" \".join(loc.split())\n",
    "\n",
    "        # Too short\n",
    "        if len(loc) <= TOO_SHORT:\n",
    "            raise ValueError\n",
    "\n",
    "        # Add it\n",
    "        return Location(loc=loc, add=1, error=0)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return Location(loc=\"\", add=0, error=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b354094b-1658-4e42-ad91-44d7160b974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    normals = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(INPUT_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from gazetteer\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        for rec in tqdm(cxn.execute(\"\"\"select * from gazetteer\"\"\"), total=total):\n",
    "            for field in FIELDS:\n",
    "                if field:\n",
    "                    loc = rec[field]\n",
    "\n",
    "                    norm = normalize_location(loc)\n",
    "                    if norm.loc:\n",
    "                        normals[norm.loc] += norm.add\n",
    "                    errors += norm.error\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        batch = [{\"locality\": k, \"hits\": v} for k, v in normals.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.to_sql(\"normalized\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b062c7-2558-4a66-9fbf-a82285b55b51",
   "metadata": {},
   "source": [
    "## Get locality words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7110400-c771-41b7-8453-722d70dc5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RE = re.compile(r\"^ [\\d.°/,\\'\\\"+-]+ \", flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9613737e-b870-452e-b682-41c5d6f52f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_proc(limit, offset):\n",
    "    errors = 0\n",
    "    all_words = defaultdict(int)\n",
    "\n",
    "    taxa_dir = Path(f_terms.__file__).parent\n",
    "    # binomial_terms = taxa_dir / \"binomial_terms.zip\"\n",
    "    monomial_terms = taxa_dir / \"monomial_terms.zip\"\n",
    "\n",
    "    taxa = tu.read_terms([monomial_terms])  # , binomial_terms])\n",
    "    taxa = {t[\"pattern\"] for t in taxa for w in t[\"pattern\"].split()}\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        rows = cxn.execute(\n",
    "            \"\"\"select * from normalized limit ? offset ?\"\"\", (limit, offset)\n",
    "        )\n",
    "\n",
    "        for phrase, hits in rows:\n",
    "            try:\n",
    "                doc = nlp(phrase)\n",
    "            except ValueError:\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            for token in doc:\n",
    "                if token.pos_ in POS_ALIAS:\n",
    "                    continue\n",
    "\n",
    "                elif token.is_punct or token.is_quote:\n",
    "                    continue\n",
    "\n",
    "                word = NUM_RE.sub(\"\", token.lower_)\n",
    "\n",
    "                if len(word) <= 1:\n",
    "                    continue\n",
    "\n",
    "                if word in SKIP:\n",
    "                    continue\n",
    "\n",
    "                if word in taxa:\n",
    "                    continue\n",
    "\n",
    "                all_words[word] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in all_words.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        csv_path = TEMP_DIR / f\"words_{offset}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f70ed2-46e8-4f61-a256-ce24875aa5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab25da1065d44b07b663eb8cb292d6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words():\n",
    "    processes = 12\n",
    "    limit = 1_000_000\n",
    "    results = []\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        cur = cxn.execute(\"\"\"select count(*) from normalized\"\"\")\n",
    "        count = cur.fetchone()[0]\n",
    "\n",
    "    total = sum(1 for _ in range(0, count, limit))\n",
    "\n",
    "    with Pool(processes=processes) as pool, tqdm(total=total) as bar:\n",
    "        for offset in range(0, count, limit):\n",
    "            results.append(\n",
    "                pool.apply_async(\n",
    "                    get_words_proc,\n",
    "                    args=(limit, offset),\n",
    "                    callback=lambda _: bar.update(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(r.get() for r in results)\n",
    "\n",
    "\n",
    "get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acea716f-790a-46a4-a243-40fe9d3beb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfc3fb84e1046d49ef8e8aaea434554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words)=1285645\n",
      "sum(words.values())=190039519\n"
     ]
    }
   ],
   "source": [
    "def write_words():\n",
    "    words = defaultdict(int)\n",
    "\n",
    "    for path in tqdm(sorted(TEMP_DIR.glob(\"words_*.csv\"))):\n",
    "        with open(path) as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "\n",
    "            for row in reader:\n",
    "                word = row[\"pattern\"]\n",
    "                words[word] += int(row[\"hits\"])\n",
    "\n",
    "    print(f\"{len(words)=}\")\n",
    "    print(f\"{sum(words.values())=}\")\n",
    "\n",
    "    batch = [{\"pattern\": k, \"hits\": v} for k, v in words.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        df.to_sql(\"words\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    df.to_csv(LOCALITIES, index=False)\n",
    "\n",
    "\n",
    "write_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dcb87-c125-476f-a102-ce00bc524e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
